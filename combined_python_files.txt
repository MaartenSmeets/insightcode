# File: helpers.py

from pathlib import Path
import re
import uuid
from datetime import datetime

def generate_unique_filename(base_name: str, extension: str) -> str:
    """Generate a unique filename with timestamp and unique ID."""
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    unique_id = uuid.uuid4().hex[:6]
    safe_base_name = re.sub(r'[^a-zA-Z0-9_\-]', '_', base_name)
    return f"{safe_base_name}_{timestamp}_{unique_id}.{extension}"

def save_output_to_file(content: str, file_path: Path):
    """Save output to a file."""
    file_path.parent.mkdir(parents=True, exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)

def is_irrelevant_file(file_path: Path) -> bool:
    """Determine if a file should be excluded from analysis."""
    # Define the maximum file size (in bytes) for source code files
    MAX_FILE_SIZE = 500 * 1024  # 500KB

    # List of irrelevant file extensions
    irrelevant_extensions = [
        # Compiled and binary files
        '.bin', '.exe', '.o', '.obj', '.class', '.pyc', '.pyo', '.jar', '.war', '.ear', '.dll', '.so', '.dylib',
        '.lib', '.a', '.whl', '.apk', '.ipa',
        # Image and media files
        '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.svg', '.webp', '.ico',
        '.mp3', '.wav', '.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm',
        # Documentation and miscellaneous files
        '.md', '.rst', '.txt', '.csv', '.tsv', '.log', '.feature', '.pdf', '.docx', '.doc', '.xls', '.xlsx',
        '.ppt', '.pptx', '.odt', '.ods', '.odp', '.rtf',
        # Archive and package files
        '.zip', '.tar', '.gz', '.tar.gz', '.tgz', '.rar', '.7z', '.bz2', '.xz', '.egg', '.gem', '.deb', '.rpm',
        # Other non-source code files
        '.swp', '.swo', '.tmp', '.cache', '.pyproj', '.csproj', '.sln', '.vcxproj',
    ]

    # List of irrelevant directories
    irrelevant_directories = [
        # Test directories
        'test', 'tests', 'spec', 'specs', 'mock', 'mocks', 'stub', 'stubs', 'fixtures', 'benchmark', 'benchmarks',
        # Version control and IDE directories
        '.git', '.svn', '.hg', '.idea', '.vscode', '__pycache__', '.tox', '.pytest_cache',
        # Build and dependency directories
        'build', 'dist', 'node_modules', 'env', 'venv', 'target', 'out', 'bin', 'obj', 'lib', 'libs',
        'generated', 'gen', 'public', 'private', 'release', 'debug', 'bower_components',
        # CI/CD and deployment directories
        '.circleci', '.github', '.gitlab', '.azure', '.vagrant', '.docker', '.dockerignore',
        # Coverage and report directories
        'coverage', 'reports', 'logs',
    ]

    # List of irrelevant filenames
    irrelevant_filenames = [
        # Common non-code files
        'LICENSE', 'LICENSE.txt', 'README', 'README.md', 'README.txt', 'CHANGES', 'CHANGELOG',
        'CONTRIBUTING', 'CODE_OF_CONDUCT', '.gitignore', '.gitattributes', 'Dockerfile',
        'Makefile', 'CMakeLists.txt', 'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',
    ]

    # Define keywords to identify test files
    test_keywords = ['test', 'spec', 'mock', 'stub', 'fixture', 'benchmark', 'ct', 'performance', 'it']

    # Check for irrelevant file extensions
    if file_path.suffix.lower() in irrelevant_extensions:
        return True

    # Check if the file is in an irrelevant directory
    if any(dir_name.lower() == part.lower() for dir_name in irrelevant_directories for part in file_path.parts):
        return True

    # Check for irrelevant filenames
    if file_path.name in irrelevant_filenames:
        return True

    # Exclude files that are test files based on filename patterns
    if any(keyword in file_path.stem.lower() for keyword in test_keywords):
        return True

    # Exclude files in directories containing test keywords
    if any(keyword in part.lower() for keyword in test_keywords for part in file_path.parts):
        return True

    # Exclude large files (e.g., files larger than 500KB)
    if file_path.is_file() and file_path.stat().st_size > MAX_FILE_SIZE:
        return True

    # Exclude build files commonly found in various languages and frameworks
    build_files = [
        # Java and related build files
        'pom.xml', 'build.gradle', 'build.gradle.kts', 'settings.gradle', 'settings.gradle.kts',
        'gradlew', 'gradlew.bat', 'mvnw', 'mvnw.cmd',
        # .NET build files
        '*.csproj', '*.vbproj', '*.fsproj', '*.sln',
        # JavaScript and TypeScript build files
        'gulpfile.js', 'gulpfile.ts', 'Gruntfile.js', 'Gruntfile.ts', 'webpack.config.js',
        'webpack.config.ts', 'rollup.config.js', 'rollup.config.ts', 'vite.config.js', 'vite.config.ts',
        # Python build files
        'setup.py', 'setup.cfg', 'pyproject.toml', 'requirements.txt', 'Pipfile', 'Pipfile.lock',
        # Ruby build files
        'Gemfile', 'Gemfile.lock', 'Rakefile',
        # PHP build files
        'composer.json', 'composer.lock',
        # Go build files
        'go.mod', 'go.sum',
        # Rust build files
        'Cargo.toml', 'Cargo.lock',
    ]

    # Check for build files
    if file_path.name.lower() in [name.lower() for name in build_files]:
        return True

    # Exclude generated files (e.g., package-lock.json)
    generated_files = [
        'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',
    ]
    if file_path.name.lower() in [name.lower() for name in generated_files]:
        return True

    return False


================================================================================

# File: config.py

from pathlib import Path

# Constants and Configuration
OUTPUT_FORMAT = 'mermaid'  # Can be 'mermaid', 'plantuml', etc.

# Ollama Configuration
OLLAMA_URL = "http://localhost:11434/api/generate"  # Configurable LLM URL
DEFAULT_SUMMARIZATION_MODEL = "deepseek-coder-v2:16b-lite-instruct-q5_K_M" # Configurable model
CLEAN_CACHE_ON_STARTUP = True  # Set to True to clean cache at startup, False to retain cache

# Directories
CACHE_DIR = Path('cache')
OUTPUT_DIR = Path('output')

# Subdirectories within OUTPUT_DIR
SUMMARIES_DIR = OUTPUT_DIR / "summaries"
UNPROCESSED_DIR = OUTPUT_DIR / "unprocessed_files"

================================================================================

# File: llm_interface.py

import logging
from pathlib import Path
from config import OLLAMA_URL, DEFAULT_SUMMARIZATION_MODEL, CACHE_DIR, OUTPUT_DIR, CLEAN_CACHE_ON_STARTUP
from helpers import save_output_to_file, generate_unique_filename, is_irrelevant_file
from file_readers import get_reader
import requests
import json
import shelve
from hashlib import md5
import shutil

# Set up logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# File Summary Prompt Template
FILE_SUMMARY_PROMPT_TEMPLATE = """You are tasked with summarizing a file from a software repository. Provide only a **precise**, **comprehensive**, and **well-structured** English summary that accurately reflects the contents of the file. Do not write or update code. Do not generate code to create a summary but create a summary. Do not ask for confirmation. Do not provide suggestions. Do not provide recommendations. Do not mention potential improvements. Do not mention considerations. Focus solely on creating the summary. Avoid redundancy and do not summarize the summary. The summary must be:

- **Factual and objective**: Include only verifiable information based on the provided file. Avoid any assumptions, opinions, interpretations, or speculative conclusions.
- **Specific and relevant**: Directly reference the actual contents of the file. Avoid general statements or unrelated information. Focus on the specific purpose, functionality, and structure of the file.
- **Concise yet complete**: Ensure that the summary captures all essential details while being succinct. Eliminate redundancy and unnecessary information.

In particular, address the following when applicable and relevant to the file’s role in the codebase. When not applicable, leave out the section:
- **Purpose and functionality**: Describe the file's core purpose, what functionality it implements, and how it fits into the broader system.
- **Key components**: Highlight any critical functions, classes, methods, or modules defined in the file and explain their roles.
- **Inputs and outputs**: Explicitly mention any input data or parameters the file processes, and describe the outputs it generates.
- **Dependencies**: Identify any internal or external dependencies (e.g., libraries, APIs, other files) and explain how they are used in the file.
- **Data flow**: Describe the flow of data through the file, including how data is processed, transformed, or manipulated.
- **Interactions**: If applicable, detail how this file interacts with other parts of the system or external systems.

Your summary should provide enough detail to give a clear understanding of the file’s purpose and its function within the codebase, without adding unnecessary explanations or speculative content.

**File being summarized**: {file_path}
{file_content}

---

Remember, your task is to summarize the **file’s content only** without generating any code or giving suggestions.
"""

# System prompt to control behavior of the LLM
SYSTEM_PROMPT = """
You are a code summarization assistant. Your task is to provide **concise, high-level summaries** of code files without including technical details, code generation, explanations, or feedback. Your summaries should describe the **purpose**, **functionality**, and **role** of each file within the overall project.

### Instructions:
- **Under no circumstances should you generate code, provide feedback, or include code snippets** in your summary. 
- **Do not explain how the code works** or suggest any improvements. 
- **Do not include any specific variable names, function names, or other technical elements**.
- Focus strictly on the **high-level purpose** and **interactions** with other parts of the project. Do not provide explanations about code structure or logic.
- **You are not allowed to modify or generate any code**.
"""

def clean_cache():
    """Clean the cache by removing the cache directory if it exists."""
    if CACHE_DIR.exists() and CACHE_DIR.is_dir():
        logging.info("Cleaning cache directory...")
        shutil.rmtree(CACHE_DIR)
        logging.info("Cache directory cleaned.")

def init_cache() -> shelve.Shelf:
    """Initialize the shelve cache."""
    logging.debug("Initializing cache directory")
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    return shelve.open(str(CACHE_DIR / 'llm_cache.db'))

def generate_cache_key(user_prompt: str, system_prompt: str, model: str) -> str:
    """Generate a unique hash for cache key based on input."""
    key_string = f"{model}_{system_prompt}_{user_prompt}"
    cache_key = md5(key_string.encode()).hexdigest()
    logging.debug(f"Generated cache key: {cache_key} for prompt: {user_prompt[:50]}")
    return cache_key

def generate_response_with_llm(user_prompt: str, system_prompt: str, model: str) -> str:
    """Call the LLM via API to generate responses with caching."""
    if CLEAN_CACHE_ON_STARTUP:
        clean_cache()
    cache = init_cache()
    cache_key = generate_cache_key(user_prompt, system_prompt, model)

    # Check if the result is already cached
    if cache_key in cache:
        logging.info(f"Fetching result from cache for prompt: {user_prompt[:50]}...")
        response_content = cache[cache_key]
        cache.close()
        return response_content

    # If not cached, call the LLM API
    try:
        logging.info(f"Sending request to LLM with model '{model}' and prompt size {len(user_prompt)}")

        # Build the full prompt as per Llama's expected format

        payload = {
            "model": model,
            "prompt": user_prompt,
            "system": system_prompt
        }

        logging.debug(f"Payload: {json.dumps(payload)}")

        headers = {'Content-Type': 'application/json'}
        response = requests.post(OLLAMA_URL, data=json.dumps(payload), headers=headers, stream=True)

        logging.debug(f"Response status code: {response.status_code}")

        if response.status_code != 200:
            logging.error(f"Failed to generate response with LLM: HTTP {response.status_code}")
            logging.debug(f"Response content: {response.text}")
            cache.close()
            return ""

        # Read the streaming response
        response_content = ""
        for line in response.iter_lines():
            if line:
                try:
                    data = json.loads(line.decode('utf-8'))
                    # Check if 'response' field is present
                    if 'response' in data:
                        response_content += data['response']
                    if data.get('done', False):
                        break
                except json.JSONDecodeError as e:
                    logging.error(f"JSONDecodeError: {e}")
                    logging.debug(f"Line content: {line}")
                    continue

        if not response_content:
            logging.warning("Unexpected response or no response.")
            logging.debug(f"Complete raw response: {response.text}")
            cache.close()
            return ""

        # Cache the result
        logging.debug("Caching the generated response.")
        cache[cache_key] = response_content
        cache.close()

        return response_content

    except Exception as e:
        logging.error(f"Failed to generate response with LLM: {e}")
        cache.close()
        raise e
    
    
def summarize_codebase(directory: Path, summarization_model: str = DEFAULT_SUMMARIZATION_MODEL) -> str:
    """Summarize the entire repository and save individual summaries with unique filenames."""

    # Create a directory for saving individual summaries
    summaries_dir = OUTPUT_DIR / "summaries"
    summaries_dir.mkdir(parents=True, exist_ok=True)

    all_files = [f for f in directory.glob('**/*') if f.is_file()]
    total_files = len(all_files)
    combined_summary = []

    logging.info(f"Starting codebase summarization... Total files to process: {total_files}")

    # Process each file and save the summaries
    for idx, file_path in enumerate(all_files, start=1):
        if is_irrelevant_file(file_path):
            logging.info(f"Skipping irrelevant file: {file_path}")
            continue

        file_extension = file_path.suffix
        reader = get_reader(file_extension)
        logging.info(f"Processing file {idx}/{total_files} ({file_path.name}) with reader for extension {file_extension}")

        # Read file content
        try:
            file_content = reader(file_path)
            logging.debug(f"Read content from file {file_path}")
        except Exception as e:
            logging.error(f"Error reading file {file_path}: {e}")
            continue

        # Prepare the prompt for summarization
        user_prompt = FILE_SUMMARY_PROMPT_TEMPLATE.format(file_path=file_path, file_content=file_content)

        # Generate the summary using the LLM
        try:
            summary = generate_response_with_llm(user_prompt, SYSTEM_PROMPT, summarization_model)
            if summary:
                # Save each summary in the summaries directory with a unique filename
                summary_filename = generate_unique_filename(file_path.stem, "txt")
                summary_file_path = summaries_dir / summary_filename
                save_output_to_file(summary, summary_file_path)
                logging.info(f"Summary saved to {summary_file_path}")

                # Add to the combined summary with the filename
                combined_summary.append(f"Filename: {file_path}\n{summary}\n")
            else:
                logging.warning(f"No summary generated for {file_path}")
        except Exception as e:
            logging.error(f"Error generating summary for file {file_path}: {e}")
            continue

        # Log progress in percentage
        progress_percentage = (idx / total_files) * 100
        logging.info(f"Progress: {progress_percentage:.2f}% ({idx}/{total_files} files processed)")

    # Combine all summaries and return
    return "\n".join(combined_summary)

================================================================================

# File: main.py

import logging
from pathlib import Path
from config import OUTPUT_DIR, DEFAULT_SUMMARIZATION_MODEL, OUTPUT_FORMAT
from helpers import generate_unique_filename, save_output_to_file
from file_readers import get_reader
from diagram_generators import generate_diagram
from llm_interface import summarize_codebase

def configure_logging():
    """Configure logging with a file handler and console output."""
    log_dir = OUTPUT_DIR
    log_file = log_dir / "script_run.log"

    # Ensure the log directory exists
    log_dir.mkdir(parents=True, exist_ok=True)

    # Clear any existing logging handlers
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    # Set up the logging configuration
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, mode='w'),  # File logging
            logging.StreamHandler()  # Console logging
        ]
    )

def main():
    """Main function to run the summarization and diagram generation process"""
    # Step 1: Configure logging
    configure_logging()

    # Log when the script starts
    logging.info("Script started.")

    try:
        # Step 2: Set repository directory path
        repo_directory = Path('repo')  # Change to your repository path

        # Step 3: Summarize the codebase
        logging.info("Starting codebase summarization...")
        codebase_summary = summarize_codebase(repo_directory, DEFAULT_SUMMARIZATION_MODEL)

        # Step 4: Check if a summary was generated
        if codebase_summary:
            logging.info("Codebase summary generated successfully.")
            
            # Step 5: Generate diagram based on the chosen output format
            logging.info(f"Generating {OUTPUT_FORMAT} diagram...")
            diagram_code = generate_diagram(codebase_summary)

            # Save the diagram file with the correct extension based on the format
            diagram_file = generate_unique_filename("architecture_diagram", OUTPUT_FORMAT)
            save_output_to_file(diagram_code, OUTPUT_DIR / diagram_file)

            logging.info(f"{OUTPUT_FORMAT.capitalize()} diagram generated and saved to {diagram_file}.")
        else:
            logging.warning("No relevant files found or summarized.")
    
    except Exception as e:
        # Log any unexpected errors
        logging.error(f"An error occurred: {e}")

    # Log when the script ends
    logging.info("Script finished.")

if __name__ == "__main__":
    main()


================================================================================

# File: codeconcat.py

import os

def combine_python_files(folder_path, output_file, exclude_folders=None):
    """
    Combines all Python files in the specified folder and its subfolders into a single file.
    The relative path of each file is added before its content.
    Folders specified in exclude_folders will be skipped.

    Args:
    folder_path (str): Path to the folder containing the Python files.
    output_file (str): Path to the output file where the combined content will be saved.
    exclude_folders (list, optional): List of folder names to exclude from the search.
                                      Defaults to excluding '.venv'.
    """

    # Set default excluded folders if none provided
    if exclude_folders is None:
        exclude_folders = ['.venv']

    with open(output_file, 'w') as outfile:
        # Walk through the folder and subfolders
        for root, dirs, files in os.walk(folder_path):
            # Skip excluded folders
            dirs[:] = [d for d in dirs if d not in exclude_folders]

            for file in files:
                if file.endswith(".py"):
                    # Get the relative path of the file
                    file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(file_path, folder_path)

                    # Write the relative path to the output file
                    outfile.write(f"# File: {relative_path}\n\n")

                    # Write the content of the file to the output file
                    with open(file_path, 'r') as infile:
                        content = infile.read()
                        outfile.write(content)

                    # Add a separator between files
                    outfile.write("\n\n" + "="*80 + "\n\n")

    print(f"All Python files from '{folder_path}' have been combined into '{output_file}'.")


# Example usage:
folder_path = './.'  # Change this to your target folder
output_file = './combined_python_files.txt'  # Change this to the desired output file path
exclude_folders = ['.venv', '__pycache__']  # Add more folders to exclude if needed
combine_python_files(folder_path, output_file, exclude_folders)


================================================================================

# File: diagram_generators/mermaid_generator.py

import logging
from pathlib import Path
from config import OUTPUT_DIR
from helpers import save_output_to_file, generate_unique_filename

# Mermaid Prompt Template
MERMAID_PROMPT_TEMPLATE = """**Objective:**
Based on the provided detailed codebase summary, generate a concise professional **Mermaid diagram** that clearly and concisely represents the system's 
architecture, major components, and data flow in a visually appealing and easy-to-understand manner. Focus on illustrating 
the **logical grouping of components**, their **interactions**, and the **data flow** between both internal and external 
systems. Make sure not to use special characters. You are only allowed in names, groupings, edges, nodes, etc., to use 
alphanumeric characters. Also avoid mentioning file extensions and function parameters. Do not use parentheses. 
Do not use quotation marks. Avoid mentioning filenames directly and use a functional name instead. Add the user as an entity 
who interacts with the analyzed code. The user should be on the left of the diagram and external dependencies on the right.

**Instructions:**

- **Generate valid Mermaid code** that accurately reflects the system architecture.
- Focus on **major components** and their **functional groupings**. Avoid mentioning individual files and solely technical components such as DAOs and configuration (unless they are an external dependency). Do not be overly detailed but stick to a high-level overview.
- Use **clear, descriptive labels** for both nodes and edges to make the diagram intuitive for stakeholders.
- **Organize components into subgraphs** or groups based on logical relationships (e.g., services, databases, external APIs) to provide a clear and structured view.
- Use **distinct but not overly bright colors** in the diagram to differentiate logical groups.
- Use a flowchart with left to right layout for enhanced readability. Inputs should be on the left and external services/systems which are called should be on the right.
- Maintain **consistent visual patterns** to distinguish between types of components.
- **Apply a minimal color scheme** to differentiate between logical groupings, system layers, or types of components, keeping the design professional.
- Use **edge labels** to describe the nature of interactions or data flow between components (e.g., "sends data", "receives response", "queries database").
- **Minimize crossing edges** and ensure proper spacing to avoid clutter and maintain clarity.
- Ensure the Mermaid syntax is correct, and the diagram can be rendered without errors.
- Avoid setting an element to be a parent of itself.
- Encapsulate all components which are part of the repository-supplied code by the name of the code. Place external components/systems inside their own encapsulation (for example systems/components like mail servers, LDAP providers, databases).

---

**Input:**  
- A comprehensive codebase summary in the form: {combined_summary}

**Your Task:**  
Generate a **well-structured and visually appealing** Mermaid diagram that illustrates the system’s architecture and functional data flows based on the provided summary. The output should be valid Mermaid code, with no extra commentary or text beyond the code itself.
"""

def generate_mermaid_code(combined_summary: str) -> str:
    """Generate Mermaid diagram based on the given codebase summary and save it to a file."""
    
    # Generate the complete prompt using the template
    prompt = MERMAID_PROMPT_TEMPLATE.format(combined_summary=combined_summary)
    
    # Generate a unique filename for the prompt
    prompt_filename = generate_unique_filename("mermaid_prompt", "txt")
    prompt_filepath = OUTPUT_DIR / prompt_filename
    
    # Save the prompt to a file in the output directory
    save_output_to_file(prompt, prompt_filepath)
    
    # Log the saved prompt location
    logging.info(f"Mermaid prompt saved to {prompt_filepath}")
    
    return prompt  # Ensure a valid string is returned


================================================================================

# File: diagram_generators/__init__.py

import importlib
import logging
from config import OUTPUT_FORMAT

def generate_diagram(combined_summary: str) -> str:
    """Dynamically load and generate a diagram based on the selected output format."""
    try:
        # Dynamically build the module name (e.g., 'mermaid_generator', 'plantuml_generator')
        module_name = f'diagram_generators.{OUTPUT_FORMAT}_generator'
        
        # Load the module dynamically
        generator_module = importlib.import_module(module_name)
        
        # Dynamically load the generator function (e.g., 'generate_mermaid_code', 'generate_plantuml_code')
        generate_function = getattr(generator_module, f'generate_{OUTPUT_FORMAT}_code')
        
        # Call the dynamically loaded function
        return generate_function(combined_summary)
    
    except ModuleNotFoundError:
        raise ValueError(f"Unsupported output format: {OUTPUT_FORMAT}")
    
    except AttributeError:
        raise ValueError(f"No valid generator function found for format: {OUTPUT_FORMAT}")


================================================================================

# File: diagram_generators/plantuml_generator.py

import logging
from pathlib import Path
from config import OUTPUT_DIR
from helpers import save_output_to_file, generate_unique_filename

# Improved PlantUML Prompt Template (without @startuml and @enduml in the prompt itself)
PLANTUML_PROMPT_TEMPLATE = """' Objective:
' Based on the provided detailed codebase summary, generate a concise and professional **PlantUML diagram** that clearly 
' represents the system's architecture, major components, and the data flow between them. Focus on illustrating:
' - The **logical grouping of components** (e.g., services, databases, APIs)
' - **Key interactions** between components and external systems
' - The **data flow** between major components

' Instructions:
' - **Generate valid PlantUML code** that accurately reflects the system architecture.
' - Focus on **major components** and their **functional groupings**.
' - **Avoid mentioning individual files** or technical components (unless external).
' - Use **clear and descriptive labels** for both nodes and interactions.
' - Organize the diagram left-to-right with inputs on the left and external systems on the right.
' - Represent external dependencies clearly (e.g., APIs, databases).
' - Maintain **consistent visual patterns** to differentiate between components, systems, and external dependencies.
' - **Use meaningful labels** to represent the nature of interactions between components.
' - Minimize edge crossings and ensure proper spacing for clarity.

' Diagram Layout:
left to right direction
actor User as user
user --> (System)

' Main components and interactions
{components}
"""

def generate_plantuml_code(combined_summary: str) -> None:
    """Generate PlantUML diagram prompt and save to a file without calling the LLM."""
    # Prepare the PlantUML prompt (without @startuml and @enduml)
    prompt = PLANTUML_PROMPT_TEMPLATE.format(components=combined_summary)

    # Save the prompt to a file in the output directory
    prompt_filename = generate_unique_filename("plantuml_prompt", "txt")
    prompt_filepath = OUTPUT_DIR / prompt_filename
    save_output_to_file(prompt, prompt_filepath)
    logging.info(f"PlantUML prompt saved to {prompt_filepath}")

    # Generate the final diagram with @startuml and @enduml tags
    diagram_code = f"@startuml\n{prompt}\n@enduml"

    # Save the final diagram to a file
    diagram_filename = generate_unique_filename("plantuml_diagram", "puml")
    diagram_filepath = OUTPUT_DIR / diagram_filename
    save_output_to_file(diagram_code, diagram_filepath)
    logging.info(f"PlantUML diagram saved to {diagram_filepath}")


================================================================================

# File: file_readers/text_reader.py

import chardet
import logging

FILE_EXTENSIONS = ['.txt', '.md', '.py', '.java', '.js', '.css', '.html', '.htm', '.c', '.cpp', '.h', '.json', '.xml', '.yml', '.yaml', '.conf', '.ini', '.log']

def read_file(file_path):
    """Read plain text files with proper encoding."""
    try:
        with open(file_path, 'rb') as file:
            raw_data = file.read(10000)
            result = chardet.detect(raw_data)
            encoding = result['encoding'] if result['encoding'] else 'utf-8'

        with open(file_path, 'r', encoding=encoding, errors='replace') as file:
            return file.read()
    except Exception as e:
        logging.error(f"Error reading text file {file_path}: {e}")
        return ""


================================================================================

# File: file_readers/pdf_reader.py

import pdfplumber
import logging
import fitz  # PyMuPDF for PDF image extraction
import pytesseract
from PIL import Image

FILE_EXTENSIONS = ['.pdf']

def read_file(file_path):
    """Extract text from a PDF file, using OCR if necessary."""
    try:
        text = ''
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text
        if not text.strip():
            # If no text was extracted, use OCR
            logging.info(f"No text extracted from {file_path}, performing OCR.")
            text = ocr_pdf_file(file_path)
        return text
    except Exception as e:
        logging.error(f"Error reading PDF file {file_path}: {e}")
        return ""

def ocr_pdf_file(file_path):
    """Perform OCR on a PDF file."""
    try:
        text = ''
        with fitz.open(file_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                pix = page.get_pixmap()
                img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                text += pytesseract.image_to_string(img)
        return text
    except Exception as e:
        logging.error(f"Error performing OCR on PDF file {file_path}: {e}")
        return ""


================================================================================

# File: file_readers/__init__.py

import os
import glob
import importlib
import logging

# Initialize a registry of readers
readers = {}

def default_reader(file_path):
    """Default text file reader if no specific reader is found for the file extension."""
    logging.debug(f"No specific reader for file extension, using default text reader for {file_path}")
    from .text_reader import read_file as read_text_file
    return read_text_file(file_path)

# Dynamically import all modules in the current directory
module_dir = os.path.dirname(__file__)
module_files = glob.glob(os.path.join(module_dir, '*.py'))

for module_file in module_files:
    module_name = os.path.basename(module_file)[:-3]  # Strip the .py extension
    if module_name == '__init__':
        continue  # Skip the __init__.py file

    try:
        # Dynamically import the module
        module = importlib.import_module(f'.{module_name}', package=__package__)

        # Check if the module has 'FILE_EXTENSIONS' and 'read_file'
        if hasattr(module, 'FILE_EXTENSIONS') and hasattr(module, 'read_file'):
            for ext in module.FILE_EXTENSIONS:
                readers[ext.lower()] = module.read_file
                logging.debug(f"Registered reader for extension {ext} from module {module_name}")
        else:
            logging.warning(f"Module {module_name} does not have FILE_EXTENSIONS or read_file function.")

    except Exception as e:
        logging.error(f"Failed to import module {module_name}: {e}")

def get_reader(file_extension):
    """Return the appropriate reader based on the file extension, or default to text reader."""
    reader = readers.get(file_extension.lower(), default_reader)
    if reader == default_reader:
        logging.info(f"No specific reader found for extension {file_extension}. Using default reader.")
    else:
        logging.info(f"Found specific reader for extension {file_extension}.")
    return reader


================================================================================

# File: file_readers/pptx_reader.py

# file_readers/pptx_reader.py
import logging
from pptx import Presentation

FILE_EXTENSIONS = ['.pptx']

def read_file(file_path):
    """Read contents from a .pptx file."""
    try:
        prs = Presentation(file_path)
        full_text = []
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    full_text.append(shape.text)
        return '\n'.join(full_text)
    except Exception as e:
        logging.error(f"Error reading pptx file {file_path}: {e}")
        return ""

================================================================================

# File: file_readers/html_reader.py

from bs4 import BeautifulSoup
import logging

FILE_EXTENSIONS = ['.html', '.htm']

def read_file(file_path):
    """Extract text from an HTML file."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            html_content = f.read()
        soup = BeautifulSoup(html_content, 'html.parser')
        return soup.get_text(separator='\n')
    except Exception as e:
        logging.error(f"Error reading HTML file {file_path}: {e}")
        return ""


================================================================================

# File: file_readers/docx_reader.py

import docx
import logging

FILE_EXTENSIONS = ['.docx']

def read_file(file_path):
    """Read contents from a .docx file."""
    try:
        doc = docx.Document(file_path)
        full_text = []
        for para in doc.paragraphs:
            full_text.append(para.text)
        return '\n'.join(full_text)
    except Exception as e:
        logging.error(f"Error reading docx file {file_path}: {e}")
        return ""


================================================================================

# File: repo/insightcode/main.py

import os 
import shelve
import uuid
import logging
import traceback
import shutil
import json
import requests
import re
from hashlib import md5
from datetime import datetime

# Import get_reader from file_readers
from file_readers import get_reader

# Constants and Configuration
OLLAMA_URL = "http://localhost:11434/api/generate"  # Configurable Ollama URL

DEFAULT_SUMMARIZATION_MODEL = 'llama3.1:8b-instruct-fp16'
SUMMARIZATION_MODEL_TOKENIZER = 'meta-llama/Llama-3.1-8B-Instruct'

CACHE_DIR = 'cache'
OUTPUT_DIR = "output"  # Single directory for all generated content
MERMAID_PROMPT_FILE = os.path.join(OUTPUT_DIR, "mermaid_prompt.txt")

# Subdirectories within OUTPUT_DIR
SUMMARIES_DIR = os.path.join(OUTPUT_DIR, "summaries")
UNPROCESSED_DIR = os.path.join(OUTPUT_DIR, "unprocessed_files")

# Logging Configuration
log_file = 'script_run.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler(log_file, mode='w'), logging.StreamHandler()]
)

# Prompt Templates
DEFAULT_MERMAID_PROMPT_TEMPLATE = """**Objective:**
Based on the provided detailed codebase summary, generate a concise professional **Mermaid diagram** that clearly and concisely represents the system's 
architecture, major components, and data flow in a visually appealing and easy-to-understand manner. Focus on illustrating 
the **logical grouping of components**, their **interactions**, and the **data flow** between both internal and external 
systems. Make sure not to use special characters. You are only allowed in names, groupings, edges, nodes, etc., to use 
alphanumeric characters. Also avoid mentioning file extensions and function parameters. Do not use parentheses. 
Do not use quotation marks. Avoid mentioning filenames directly and use a functional name instead. Add the user as an entity 
who interacts with the analysed code. The user should be on the left of the diagram and external dependencies on the right.

**Instructions:**

- **Generate valid Mermaid code** that accurately reflects the system architecture.
- Focus on **major components** and their **functional groupings**. Avoid mentioning individual files and solely technical components such as DAOs and configuration (unless they are an external dependency). Do not be overly detailed but stick to a high-level overview.
- Use **clear, descriptive labels** for both nodes and edges to make the diagram intuitive for stakeholders.
- **Organize components into subgraphs** or groups based on logical relationships (e.g., services, databases, external APIs) to provide a clear and structured view.
- Use **distinct but not overly bright colors** in the diagram to differentiate logical groups.
- Use a flowchart with left to right layout for enhanced readability. Inputs should be on the left and external services/systems which are called should be on the right.
- Maintain **consistent visual patterns** to distinguish between types of components.
- **Apply a minimal color scheme** to differentiate between logical groupings, system layers, or types of components, keeping the design professional.
- Use **edge labels** to describe the nature of interactions or data flow between components (e.g., "sends data", "receives response", "queries database").
- **Minimize crossing edges** and ensure proper spacing to avoid clutter and maintain clarity.
- Ensure the Mermaid syntax is correct, and the diagram can be rendered without errors.
- Avoid setting an element to be a parent of itself.
- Encapsulate all components which are part of the repository supplied code by the name of the code. Place external components/systems inside their own encapsulation (for example systems/components like mailservers, LDAP providers, databases).

---

**Input:**  
- A comprehensive codebase summary in the form: {combined_summary}

**Your Task:**  
Generate a **well-structured and visually appealing** Mermaid diagram that illustrates the system’s architecture and functional data flows based on the provided summary. The output should be valid Mermaid code, with no extra commentary or text beyond the code itself.
"""

FILE_SUMMARY_PROMPT_TEMPLATE = """You are tasked with summarizing a file from a software repository. Provide only a **precise**, **comprehensive**, and **well-structured** **English language** summary that accurately reflects the contents of the file. Do not write or update code. Do not generate code to create a summary but create a summary. Do not ask for confirmation. Do not provide suggestions. Do not provide recommendations. Do not mention potential improvements. Do not mention considerations. Do not mention what you are not certain of. Do not mention the document or file. Focus solely on creating the summary. Avoid redundancy and do not summarize the summary. The summary must be:

- **Factual and objective**: Include only verifiable information based on the provided file. Avoid any assumptions, opinions, interpretations, or speculative conclusions.
- **Specific and relevant**: Directly reference the actual contents of the file. Avoid general statements or unrelated information. Focus on the specific purpose, functionality, and structure of the file.
- **Concise yet complete**: Ensure that the summary captures all essential details while being succinct. Eliminate redundancy and unnecessary information.

In particular, address the following when applicable and relevant to the file’s role in the codebase. When not applicable, leave out the section:
- **Purpose and functionality**: Describe the file's core purpose, what functionality it implements, and how it fits into the broader system.
- **Key components**: Highlight any critical functions, classes, methods, or modules defined in the file and explain their roles.
- **Inputs and outputs**: Explicitly mention any input data or parameters the file processes, and describe the outputs it generates.
- **Dependencies**: Identify any internal or external dependencies (e.g., libraries, APIs, other files) and explain how they are used in the file.
- **Data flow**: Describe the flow of data through the file, including how data is processed, transformed, or manipulated.
- **Interactions**: If applicable, detail how this file interacts with other parts of the system or external systems.

Your summary should provide enough detail to give a clear understanding of the file’s purpose and its function within the codebase, without adding unnecessary explanations or speculative content.

**File being summarized**: {file_path}
{file_content}

***Your task**
Your goal is to only create a summary in English of the file's purpose, functionality, key components, inputs/outputs, dependencies, data flow, and interactions, ensuring the summary is factual, specific, relevant, and concise.
"""

# Helper Functions
def replace_special_statements(text):
    pattern = r'(\|\s*([^\|]+)\s*\|)|(\[\s*([^\]]+)\s*\])|(\{\s*([^\}]+)\s*\})'

    def replace_func(match):
        if match.group(1):  # If the match is within | |
            content = match.group(2)
        elif match.group(3):  # If the match is within [ ]
            content = match.group(4)
        else:  # If the match is within { }
            content = match.group(6)
        
        cleaned_content = ''.join(word.capitalize() for word in re.findall(r'\w+', content))
        
        if match.group(1):  # Replace | |
            return f"|{cleaned_content}|"
        elif match.group(3):  # Replace [ ]
            return f"[{cleaned_content}]"
        else:  # Replace { }  
            return f"{{{cleaned_content}}}"
    
    return re.sub(pattern, replace_func, text)

def generate_unique_filename(base_name: str, extension: str) -> str:
    """Generate a unique filename with timestamp and unique ID."""
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    unique_id = uuid.uuid4().hex[:6]
    safe_base_name = re.sub(r'[^a-zA-Z0-9_\-]', '_', base_name)  # Make the base name safe for filenames
    return f"{safe_base_name}_{timestamp}_{unique_id}.{extension}"

def init_cache() -> shelve.Shelf:
    """Initialize the shelve cache."""
    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)
    return shelve.open(os.path.join(CACHE_DIR, 'llm_cache.db'))

def generate_cache_key(prompt: str, model: str) -> str:
    """Generate a unique hash for cache key based on input."""
    key_string = f"{model}_{prompt}"
    return md5(key_string.encode()).hexdigest()

def generate_response_with_ollama(prompt: str, model: str) -> str:
    """Call the LLM via Ollama to generate responses with caching."""
    cache = init_cache()
    cache_key = generate_cache_key(prompt, model)

    # Check if the result is already cached
    if cache_key in cache:
        logging.info(f"Fetching result from cache for prompt: {prompt[:50]}...")
        response_content = cache[cache_key]
        cache.close()
        return response_content

    try:
        logging.debug(f"Sending request to Ollama with model '{model}' and prompt size {len(prompt)}")
        url = OLLAMA_URL
        payload = {"model": model, "prompt": prompt}
        headers = {'Content-Type': 'application/json'}

        # Send the request
        response = requests.post(url, data=json.dumps(payload), headers=headers, stream=True)

        if response.status_code != 200:
            logging.error(f"Failed to generate response with Ollama: HTTP {response.status_code}")
            cache.close()
            return ""

        response_content = ''
        for line in response.iter_lines():
            if line:
                try:
                    line_json = json.loads(line.decode('utf-8'))
                    response_text = line_json.get('response', '')
                    response_content += response_text
                except json.JSONDecodeError as e:
                    logging.error(f"Failed to parse line as JSON: {e}")
                    continue

        if not response_content:
            logging.warning("Unexpected response or no response.")
            cache.close()
            return ""

        # Cache the result
        cache[cache_key] = response_content
        cache.close()

        return response_content

    except Exception as e:
        logging.error(f"Failed to generate response with Ollama: {e}")
        logging.debug(f"Prompt used: {prompt[:200]}...")
        logging.debug(f"Traceback: {traceback.format_exc()}")
        cache.close()
        raise e

def extract_mermaid_code(content: str) -> str:
    """Extract Mermaid code from LLM output."""
    lines = content.strip().splitlines()

    # Remove leading and trailing ```
    if lines and lines[0].strip().startswith("```"):
        lines = lines[1:]
    if lines and lines[-1].strip().startswith("```"):
        lines = lines[:-1]

    return "\n".join(lines).strip()

def generate_mermaid_code(combined_summary: str, mermaid_context: str) -> str:
    """Generate the Mermaid code based on the combined summary."""
    llm_mermaid_prompt = mermaid_context.format(combined_summary=combined_summary)

    # Save the Mermaid prompt context to a file for debugging
    save_output_to_file(llm_mermaid_prompt, MERMAID_PROMPT_FILE)

    # Generate Mermaid code
    mermaid_code = generate_response_with_ollama(llm_mermaid_prompt, DEFAULT_SUMMARIZATION_MODEL)

    # Extract Mermaid code from the response
    mermaid_code = extract_mermaid_code(mermaid_code)

    # Save the Mermaid code to a file
    mermaid_file = os.path.join(OUTPUT_DIR, "architecture_diagram.mmd")
    save_output_to_file(mermaid_code, mermaid_file)

    logging.info(f"Mermaid diagram generated and saved to {mermaid_file}")

    return mermaid_code

def summarize_codebase(directory: str, summarization_model: str) -> str:
    """Summarize the entire repository."""
    all_files = list_all_files(directory)
    total_files = len(all_files)
    codebase_summary = []
    logging.info(f"Total files to process: {total_files}")

    # Create OUTPUT_DIR and subdirectories
    os.makedirs(SUMMARIES_DIR, exist_ok=True)
    os.makedirs(UNPROCESSED_DIR, exist_ok=True)

    for idx, file_path in enumerate(all_files, start=1):
        logging.info(f"Processing file {idx}/{total_files}: {file_path}")
        file_content = read_file(file_path, directory, UNPROCESSED_DIR)

        if not file_content:
            logging.warning(f"Skipping unreadable or empty file: {file_path}")
            continue

        try:
            summary, is_test_file_flag = generate_summary(file_path, file_content, summarization_model)

            if is_test_file_flag:
                logging.info(f"Test or irrelevant file detected and skipped: {file_path}")
                continue

            if summary:
                cleaned_summary = clean_generated_summary(summary)
                formatted_summary = f"File: {file_path}\n\n{cleaned_summary}\n"

                # Generate a safe filename for the summary
                file_summary_name = re.sub(r'[^a-zA-Z0-9_\-]', '_', os.path.basename(file_path)) + "_summary.txt"

                # Treat all summaries as relevant and save to SUMMARIES_DIR
                codebase_summary.append(formatted_summary)
                save_output_to_file(formatted_summary, os.path.join(SUMMARIES_DIR, file_summary_name))

            if idx % 5 == 0 or idx == total_files:
                logging.info(f"Progress: {idx}/{total_files} files processed.")

        except Exception as e:
            logging.error(f"Error processing file: {file_path}")
            logging.error(f"Exception details: {str(e)}")
            logging.error(f"Traceback: {traceback.format_exc()}")

            copy_unreadable_file(file_path, directory, UNPROCESSED_DIR)

    combined_summary = "\n".join(codebase_summary)

    if combined_summary:
        logging.info("Final codebase summary generated.")
        summary_file = os.path.join(OUTPUT_DIR, "codebase_summary.txt")
        save_output_to_file(combined_summary, summary_file)
    else:
        logging.warning("No relevant summaries generated.")

    return combined_summary

def generate_summary(file_path: str, file_content: str, summarization_model: str) -> tuple:
    """Generate a summary for each file."""
    if is_test_file(file_path):
        logging.info(f"Skipping test file: {file_path}")
        return None, True

    if len(file_content.strip()) == 0:
        logging.warning(f"Skipping empty file: {file_path}")
        return None, True

    # Prepare the prompt
    prompt = FILE_SUMMARY_PROMPT_TEMPLATE.format(
        file_path=file_path,
        file_content=file_content
    )

    # Generate the summary
    summary = generate_response_with_ollama(prompt, summarization_model)
    return clean_generated_summary(summary), False

def clean_generated_summary(summary: str) -> str:
    """Clean and format the final summary."""
    cleaned_summary = "\n".join(
        [sentence for sentence in summary.split("\n") if not sentence.startswith("Let me know")]
    )
    return cleaned_summary.rstrip()

def read_file(file_path: str, base_directory: str, unprocessed_directory: str) -> str:
    """Read the contents of a file using the appropriate reader module."""
    try:
        _, file_extension = os.path.splitext(file_path)
        reader = get_reader(file_extension)
        return reader(file_path)
    except Exception as e:
        logging.error(f"Error reading {file_path}: {e}")
        copy_unreadable_file(file_path, base_directory, unprocessed_directory)
        return ""

def save_output_to_file(content: str, file_name: str):
    """Save output to a file."""
    os.makedirs(os.path.dirname(file_name), exist_ok=True)
    with open(file_name, 'w', encoding='utf-8') as f:
        f.write(content)

def list_all_files(directory: str) -> list:
    """List all relevant files in the directory."""
    all_files = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            file_path = os.path.join(root, file)
            if is_relevant_file(file_path):
                all_files.append(file_path)
    return all_files

def copy_unreadable_file(file_path: str, base_directory: str, unprocessed_directory: str):
    """Copy unreadable files to a new directory while maintaining relative paths."""
    relative_path = os.path.relpath(file_path, base_directory)
    dest_path = os.path.join(unprocessed_directory, relative_path)
    os.makedirs(os.path.dirname(dest_path), exist_ok=True)
    shutil.copy2(file_path, dest_path)
    logging.info(f"Copied unreadable file {file_path} to {dest_path}")

def is_test_file(file_path: str) -> bool:
    """Determine if the file is a test file."""
    file_path_lower = file_path.lower()
    test_indicators = [
        "src/it",
        "src/performance",
        "src/ct",
        "src/test",
        "test/resources",
        "test/java",
        "test",
        "/tests/",
        "\\tests\\",
        "/test/",
        "\\test\\",
        "test_",
        "_test",
        "spec/",
        "spec\\",
        "specs/",
        "specs\\",
        "/spec/",
        "\\spec\\",
    ]
    for indicator in test_indicators:
        if indicator in file_path_lower:
            return True

    return False

def is_relevant_file(file_path: str) -> bool:
    """Determine if a file is relevant to process."""
    # Exclude test files
    if is_test_file(file_path):
        return False
    # Exclude specific files
    EXCLUDED_FILES = [
        'pom.xml', 'jenkinsfile', 'build.gradle', 'package.json', 'package-lock.json',
        'yarn.lock', 'Makefile', 'Dockerfile', 'README.md', 'LICENSE', 'CONTRIBUTING.md',
        '.gitignore', 'gradlew', 'gradlew.bat', 'mvnw', 'mvnw.cmd', 'setup.py',
        'requirements.txt', 'environment.yml', 'Pipfile', 'Pipfile.lock', 'Gemfile', 
        'Gemfile.lock', '.gitlab-ci.yml', 'renovate.json', 'Dockerfile', 'docker-compose.yml',
        'bootstrap.min.css'
    ]
    if os.path.basename(file_path).lower() in EXCLUDED_FILES:
        return False
    # Include files with relevant extensions
    RELEVANT_EXTENSIONS = [
        '.java', '.kt', '.xml', '.yml', '.yaml', '.properties', '.conf', '.sql', '.json',
        '.js', '.ts', '.tsx', '.jsx', '.py', '.rb', '.go', '.php', '.cs', '.cpp', '.c',
        '.h', '.swift', '.rs', '.erl', '.ex', '.exs', '.html', '.htm', '.txt', '.md', '.docx', '.pdf'
    ]
    _, file_extension = os.path.splitext(file_path)
    if file_extension.lower() in RELEVANT_EXTENSIONS:
        return True
    return False

def main():
    # Define constants for the directory and models
    directory = 'repo'  # Change to your repository path
    summarization_model = DEFAULT_SUMMARIZATION_MODEL
    mermaid_context = DEFAULT_MERMAID_PROMPT_TEMPLATE

    # Run summarization
    codebase_summary = summarize_codebase(
        directory,
        summarization_model
    )

    if codebase_summary:
        # Generate Mermaid code
        generate_mermaid_code(
            codebase_summary,
            mermaid_context
        )
    else:
        logging.warning("No files found or summarized.")

if __name__ == "__main__":
    main()


================================================================================

# File: repo/insightcode/file_readers/text_reader.py

import chardet
import logging
import os
import re

FILE_EXTENSIONS = ['.txt', '.md', '.py', '.java', '.js', '.css', '.c', '.cpp', '.h', '.json', '.xml', '.yml', '.yaml', '.conf', '.ini', '.log']

COMMENT_SYNTAX = {
    '.py':    {'single_line': ['#'], 'multi_line': [('"""', '"""'), ("'''", "'''")]},
    '.java':  {'single_line': ['//'], 'multi_line': [('/*', '*/')]},
    '.js':    {'single_line': ['//'], 'multi_line': [('/*', '*/')]},
    '.c':     {'single_line': ['//'], 'multi_line': [('/*', '*/')]},
    '.cpp':   {'single_line': ['//'], 'multi_line': [('/*', '*/')]},
    '.h':     {'single_line': ['//'], 'multi_line': [('/*', '*/')]},
    '.css':   {'single_line': [],     'multi_line': [('/*', '*/')]},
    '.xml':   {'single_line': [],     'multi_line': [('<!--', '-->')]},
    '.yml':   {'single_line': ['#'],  'multi_line': []},
    '.yaml':  {'single_line': ['#'],  'multi_line': []},
    '.conf':  {'single_line': ['#', ';'], 'multi_line': []},
    '.ini':   {'single_line': ['#', ';'], 'multi_line': []},
    '.sh':    {'single_line': ['#'],  'multi_line': []},
}

def remove_comments(code, file_extension):
    """Remove comments from code based on file extension."""
    syntax = COMMENT_SYNTAX.get(file_extension, None)
    if not syntax:
        # If no comment syntax is defined for this extension, return code as is
        return code

    single_line_markers = syntax.get('single_line', [])
    multi_line_markers = syntax.get('multi_line', [])

    # Remove multi-line comments
    for start_marker, end_marker in multi_line_markers:
        pattern = re.escape(start_marker) + r'(.*?)' + re.escape(end_marker)
        code = re.sub(pattern, '', code, flags=re.DOTALL)

    # Remove single-line comments
    for marker in single_line_markers:
        pattern = re.escape(marker) + r'.*?$'
        code = re.sub(pattern, '', code, flags=re.MULTILINE)

    return code

def read_file(file_path):
    """Read plain text files with proper encoding and remove unnecessary content."""
    try:
        with open(file_path, 'rb') as file:
            raw_data = file.read(10000)
            result = chardet.detect(raw_data)
            encoding = result['encoding'] if result['encoding'] else 'utf-8'

        with open(file_path, 'r', encoding=encoding, errors='replace') as file:
            code = file.read()

        file_extension = os.path.splitext(file_path)[1]

        code = remove_comments(code, file_extension)

        return code
    except Exception as e:
        logging.error(f"Error reading text file {file_path}: {e}")
        return ""


================================================================================

# File: repo/insightcode/file_readers/pdf_reader.py

import pdfplumber
import logging
import fitz  # PyMuPDF for PDF image extraction
import pytesseract
from PIL import Image

FILE_EXTENSIONS = ['.pdf']

def read_file(file_path):
    """Extract text from a PDF file, using OCR if necessary."""
    try:
        text = ''
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text
        if not text.strip():
            # If no text was extracted, use OCR
            logging.info(f"No text extracted from {file_path}, performing OCR.")
            text = ocr_pdf_file(file_path)
        return text
    except Exception as e:
        logging.error(f"Error reading PDF file {file_path}: {e}")
        return ""

def ocr_pdf_file(file_path):
    """Perform OCR on a PDF file."""
    try:
        text = ''
        with fitz.open(file_path) as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                pix = page.get_pixmap()
                img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                text += pytesseract.image_to_string(img)
        return text
    except Exception as e:
        logging.error(f"Error performing OCR on PDF file {file_path}: {e}")
        return ""


================================================================================

# File: repo/insightcode/file_readers/__init__.py

import os
import glob
import importlib
import logging

# Initialize a registry of readers
readers = {}

def default_reader(file_path):
    logging.debug(f"No specific reader for file extension, using default text reader for {file_path}")
    from .text_reader import read_file as read_text_file
    return read_text_file(file_path)

# Dynamically import all modules in the current directory
module_dir = os.path.dirname(__file__)
module_files = glob.glob(os.path.join(module_dir, '*.py'))

for module_file in module_files:
    module_name = os.path.basename(module_file)[:-3]
    if module_name == '__init__':
        continue
    module = importlib.import_module(f'.{module_name}', package=__package__)
    if hasattr(module, 'FILE_EXTENSIONS') and hasattr(module, 'read_file'):
        for ext in module.FILE_EXTENSIONS:
            readers[ext.lower()] = module.read_file

def get_reader(file_extension):
    return readers.get(file_extension.lower(), default_reader)


================================================================================

# File: repo/insightcode/file_readers/pptx_reader.py

import logging
from pptx import Presentation

FILE_EXTENSIONS = ['.pptx']

def read_file(file_path):
    """Read contents from a .pptx file."""
    try:
        # Load the PowerPoint presentation
        presentation = Presentation(file_path)
        full_text = []

        # Iterate through slides and extract text from each shape
        for slide in presentation.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):  # Check if the shape has text attribute
                    full_text.append(shape.text)

        return '\n'.join(full_text)
    
    except Exception as e:
        logging.error(f"Error reading pptx file {file_path}: {e}")
        return ""


================================================================================

# File: repo/insightcode/file_readers/html_reader.py

from bs4 import BeautifulSoup
import logging

FILE_EXTENSIONS = ['.html', '.htm', '.xhtml']

def read_file(file_path):
    """Extract text from an HTML file."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            html_content = f.read()
        soup = BeautifulSoup(html_content, 'html.parser')
        return soup.get_text(separator='\n')
    except Exception as e:
        logging.error(f"Error reading HTML file {file_path}: {e}")
        return ""


================================================================================

# File: repo/insightcode/file_readers/docx_reader.py

import docx
import logging

FILE_EXTENSIONS = ['.docx']

def read_file(file_path):
    """Read contents from a .docx file."""
    try:
        doc = docx.Document(file_path)
        full_text = []
        for para in doc.paragraphs:
            full_text.append(para.text)
        return '\n'.join(full_text)
    except Exception as e:
        logging.error(f"Error reading docx file {file_path}: {e}")
        return ""


================================================================================

